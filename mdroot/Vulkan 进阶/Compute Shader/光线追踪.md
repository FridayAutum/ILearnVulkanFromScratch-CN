# 光线追踪

在本章节中我们将着重探讨如何使用 Compute Shader 实现一个基本的光线追踪阴影和镜面反射机制。该案例中没有专属硬件和设备扩展被用来进行光线追踪运算。使用 NVIDIA GPU 进行专属硬件加速的光线追踪案例将会在后面讨论。

### 简介

*注：理论上，我觉得将光线追踪和传统方法作比较的最学术派的做法就是甩一个渲染方程然后分析基于光栅化的种种 trick 和 hack 对渲染方程做了哪些假设和近似，但由于这篇笔记不是讲实时渲染技术的，因此这里试图换一种角度来引入光线追踪。*

在最初，图形管线引以为傲的 Rasterization 光栅化机制将屏幕上的每一个像素所对应的物体片元的位置和朝向插值计算出来，并且我们知道灯光的位置/朝向和摄像机的位置，有了这些因素我们能够使用一个近似光照模型来模拟光线从不同类型光源照到物体表面并根据材质属性反射到摄像机视点的颜色值。如果场景中有多个灯光，那就独立点亮每个灯光渲染一遍场景，把所有灯光对片元的贡献值加起来即可。需要渲染的光太多遮挡太厉害，每次都需要重新渲染场景？没有关系，我们将所有需要运算的中间值（世界空间位置、深度、法线、物体自身的颜色属性）记录下来，并且按照光源作用范围 Invoke 需要的着色器来渲染，最终 Blend（Add）到 Render Target 上。

这种方法看似很完美，因为我们成功的将这么复杂的物理现象以高度并行化的方式快速模拟了出来，并且最终结果看起来非常诱人。光栅化的过程感觉其实就是从所有屏幕像素都射出一道射线到物体上（片元世界空间位置 -> 摄像机位置），把这道射线当成反射射线，把光照射的方向（光源 -> 片元世界空间位置  / 平行光方向）当成入射射线并套用光照模型计算，而这个过程不就是**追踪了一束光源 -> 片元世界空间位置 -> 摄像机位置**的光吗！

但我们突然意识到，这样做阴影没法计算了：我们一直用的是光源 -> 片元世界空间位置，而这条光路其中会不会被障碍物遮挡从而导致光根本找不到这个地儿呢（想想 BRDF 中的 Visibility 项）？于是我们决定增加一种观察场景的角度，即光源视角，提出了 Shadow Mapping 的技术。世界上没有零体积的“点光源”，也没有完美的“平行光”，这些光源种类只是我们的美好模型而已。由于光源有体积所产生的“软阴影”怎么模拟？我们可以对 Shadow Map 进行 PCF 滤波做抗锯齿和软阴影，使用孪生兄弟 PCSS 和 VSSM、MSM 等技术来模拟不同位置的软硬程度 。

材质不一定全是粗糙的。生活中常见的反射效果怎么办？我们将物体沿着反射面镜面复制一份到底下来模拟，或者开一个对称的摄像机渲染到平面或 Cubemap，或者用预渲染甚至实时渲染的 Cubemap 模拟，或者用屏幕空间反射 SSR 来模拟（仔细想想，SSR 的原理不就是在屏幕空间信息中做 Path Tracing 吗），或者用 Planar Reflection 技术将屏幕像素投影到平面做平面反射……做一些简单的 hack，世界依然和平。

但光源照射到物体后不只会反射到摄像机里，还会在空间反复 bounce，最终才被摄像机记录下来，我们称之为**“全局光照”**。这种物理现象我们并没有能力使用传统的光源 -> 片元世界空间位置 -> 摄像机的传统方法计算。但是我们已经非常依赖传统的光栅化，并且光栅化有着无与伦比的性能优势，我们只好继续在模拟的路上前行：着色时的 Cubemap、Spherical Harmonics 模拟环境光，光源空间的 RSM、物体表面的 Lightmap 光照烘焙，世界空间的 LPV、 VXGI 和基于距离场 Distance Field 的一系列应用，屏幕空间的 SSAO、SSGI 等等模拟的算法和技术。这些方法仍然基于光栅化之上，享受着光栅化带来的效率，同时一直在贯彻图形学第一定律：**如果它看起来是对的，那么它就是对的**。结合上这些 Eye Catching 的技术，有时候甚至使得渲染出的场景比真实场景更加诱人，更加富有艺术感。

仔细想想，如果一种奠基的方法为了实现某些功能，需要不断对其提出修改和扩充，用一些 hack 来实现，那么这种奠基方法的可使用性就有待考量了。我们为了模拟光线的真实行为，使用了那么多的 hack 来实现出**“阴影”、“反射”和“全局光照”**。而这些 hack 也越来越拖累光栅化所具有的效率，导致着我们不得不在精度和分辨率上做文章，导致在低端设备上渲染出的结果光怪陆离，看起来并不是那么对了。所以，从光栅化提出的时候，真实感渲染方面就一直有人试图做这么一件事儿：直接模拟光线在环境中的吸收/反射/折射等等光线传播行为，即这一章节的主角：光线追踪，Ray Tracing。这么做相当可行，并且能够产生和真实世界并无二至的结果，但其弊端就是过于消耗运算性能，导致其只能在渲染农场中离线渲染个几个月才能呈现出一段视频出来，想在实时的游戏中使用更是天方夜谭。

所以光线追踪为什么那么消耗资源呢？这得从光线追踪的原理谈起了：

* 

* 在真实生活中摄像机传感器肯定不是一个点，但我们在这里为了计算方便考虑成一个点，该点即是视锥体四条边的汇聚点，在初次发射光线时按照该像素的屏幕位置根据视锥体的属性计算出该射线的方向并“发射”该射线直到其接触到某物体表面（closest hit）。接着，我们向所有光源发射光线，如果中间不受遮挡直接到那就取该光源的颜色值，受遮挡则不贡献值（无效）。根据**光路可逆**的大原则，这种逆向从摄像机发射光线并跟踪到光源的方法可行。这种方法实现了**直接光照**，效果和光栅化一样，因为上面提到了，**光栅化可以看成是不带光线反弹的光线追踪**。是的，很多情况下光线追踪渲染器的实现也是直接使用光栅化加速从摄像机到场景的这一光路的计算，并且实时光线追踪中的降噪算法也会用到 G-Buffer 中光栅化提供给我们的深度和法线 Buffer 指导降噪过程。
* 同时，因为该片元的颜色值还受到**间接光照**，即光源照到其它物体反弹（bounce）到这个片元上的光照，甚至如果该物体为一个理想的平滑表面时，直接光照将不再贡献值，而完全由反射光决定颜色，即一个镜面。所以我们除了考虑直接光照外，要再生成一个或多个光线逆着间接光照的光路在空间中继续传播，每碰到一个表面再追踪光源并继续间接光照的发散，循环往复。但由于首先真实情况下光线会衰减，不会永久弹跳下去，其次是如果我们一直弹跳，算法就不会停止（按照定义这就不是算法了），所以一段时间过后，这跟光线再落到某个物体表面的时候，我们便仅对其计算直接光照。若光源在反弹过程中的入射光线直接射到无穷远的场景外了（即没有碰撞），这根光路便被认为时无效的（ray miss）。同时，因为光源在所谓“反射”的时候并不是永远镜面反射，所以我们要在一个出射光源逆着光路到物体表面时模拟出多个入射光源模拟漫反射，最终出射颜色值为入射贡献的所有颜色值的和（Final Gathering）。
* 这种追踪行为本质上构建了一棵树并进行递归。每条光路在传播时会记录其所属像素和中间由于反弹或衰减的量（若有的话），并在其最终到达光源时考虑上衰减对其所属的像素值直接贡献颜色；或者不按光路，而是按照射线来回溯，将颜色值的贡献逐层传递到根，即屏幕像素上。

> The basic idea is to trace rays from the camera, back into the scene, and note where these “eye rays” end up. At each intersection point, you can trace another ray to all the light sources in the scene, and if they don’t hit another object, then the light form that source is contributed to the pixel. There might be other light which is contributed from the reflection or transmission direction, so you basically recurse: you send another ray, similar to your eye ray, but originating at the intersection point and heading off in the transmitted or reflected direction. Thus, the final color of the eye ray is a weighted sum of a tree of different results of traced rays in the scene.

可以看出，上面很多很多因素在影响着光线追踪的计算量，而这些计算量必定直接和性能挂钩：**光线是否反弹和反弹次数**，**光线一次反弹发散的个数**，并且还有一个致命的问题：**BVH 碰撞检测**。但这种方法在原理上已经是最优的了：如果我们真的顺着光路让灯光发射数以万计的射线，并且对每一根光线计算其行为，结果是根本没有几根光线恰好照到在视锥体范围内到达摄像机的原点上，白白浪费了性能。

因此，虽然光线追踪提出了更符合物理的光线传播模型，但其带来的性能牺牲恐怕还没有光栅化那些 hack 的零头多。并且由于光栅化流水线的工作原理和光线追踪完全不同，将工作流程从光栅化完全切换到光线追踪也不现实。所以我们在现阶段硬件的运算性能上，不用光线追踪彻底取代光栅化，而是**取光线追踪的本质和原语**：光的发射 ray gen、光的碰撞和再生成 ray hit，光的无效 ray miss，并结合上光栅化来实现某些仅靠光栅化无法完美达到的效果：

（这里使用 NVIDIA 的 RTX 介绍页面：https://www.nvidia.com/en-us/geforce/news/geforce-gtx-dxr-ray-tracing-available-now/，具体实现可以参考 Turing 架构的白皮书）

* 阴影：仅计算直接光照，不使用任何光线弹跳算间接光照。

  ![How DXR Shadows work](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/geforce-dxr-ray-tracing/shadows-dxr-explainer-850px.jpg)

* 镜面反射：不计算直接光照，仅计算一次镜面反射。

  ![How DXR Reflections work](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/geforce-dxr-ray-tracing/reflections-dxr-explainer-850px.jpg)

* 高级反射：不计算直接光照，并计算多次光线反射。

![How Advanced DXR Reflections work](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/geforce-dxr-ray-tracing/advanced-reflections-dxr-explainer-850px.jpg)

* 全局光照：最接近光线追踪的最终效果，直接间接光照都开启。同时，我们可以充分假设并利用粗糙反射面的反射方向随机（Diffuse BRDF）的特性，每次只发射一根或很少的几根射线随机（注意，此处依然仅适合 Diffuse BRDF）一个方向进行间接光照，并在之后结合着之前的结果进行降噪，解决了光线追踪时光线指数级别的增长，这种方法被称为 Path Tracing。使用 Path Tracing 在大空间和粗糙表面时的性能相当的好，并且和传统 Ray Tracing 甚至真实照片比较时的信噪比也比较大，可以达到拟真的全局光照效果。

  ![How DXR Global Illumination works](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/geforce-dxr-ray-tracing/global-illumination-dxr-explainer-850px.jpg)

