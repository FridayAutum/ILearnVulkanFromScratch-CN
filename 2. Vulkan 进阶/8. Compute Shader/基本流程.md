## 基本流程

这里使用 Sascha 的教程 https://github.com/SaschaWillems/Vulkan/tree/master/examples/computeshader ，使用 Compute Shader 将一张图像进行图像处理（锐化、边缘检测、浮雕），然后将原图和处理后的图像作为一个 quad 的纹理在图形流水线上绘制。这里为了演示，每一帧更新一次处理后的图像。

### 计算队列

首先我们要从逻辑设备中获取一个计算队列，原则是优先选择独立的 Compute 队列，若不存在的话就找到一个支持 Compute 的队列（往往和 Graphics 和 Transfer 相同）。寻找方法于之前相同，这里也贴出代码：

```c++
// 独立 Compute 队列的检测
if ((queueFamilyProperties[i].queueFlags & VK_QUEUE_COMPUTE_BIT) &&
    ((queueFamilyProperties[i].queueFlags & VK_QUEUE_GRAPHICS_BIT) == 0))
...
```

由于支持 Vulkan API 的图形设备必须支持通用计算，因此不用做不支持的 fallback。同时一般独立显卡都存在独立的 Compute 队列。使用一个独立的 Compute 队列可以并行进行通用计算和图形的任务（见概念汇总中“同步”一章），从而实现 Async Compute，详见概念汇总中“队列和队列类型”一章。

### 资源

* 原图：正常读取即可，但因为要供通用计算使用，因此 usage 指定为 `VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_STORAGE_BIT`，layout 转换为 `GENERAL`。

* 存储渲染结果的图像：图像的创建方法还是遵守着三部曲：Image, ImageView, Sampler，当然这里不用牵扯到任何将图像数据复制进去的问题。在创建出 image 后，需要将 layout 从 `UNDEFINED` 转换为 `GENERAL`，供通用计算使用。

  注意，原图和结果图在创建时都可以指定 sharing mode 为 `VK_SHARING_MODE_EXCLUSIVE`，因为我们不需要在不同队列同时对这些图进行修改，而是会通过 barrier 确保计算队列写入完毕后再在图像队列读取。当然我们可以指定 `VK_SHARING_MODE_CONCURRENT`，但是这么做一是没有必要，二是会显著影响性能（其在某些驱动上关闭了 DCC，因此消耗带宽）。

* Pipeline Layout：在设置 Compute Shader 需要的输入/输出图像时，都要将 `VkDescriptorSetLayoutBinding` 中的 `type` 指定为 `STORAGE_IMAGE`。注意此处不是之前一直使用的 `COMBINED_IMAGE_SAMPLER` ，即采样器，因为我们首先无法获得所谓“纹理坐标”（并没有固定输入输出）所以并不能提供给采样器。同时，在 Compute Shader 中我们希望直接得到图像中精准的像素坐标 [0, width), [0, height)，并且对具体的像素进行读写操作，因此传 image 比传 sampler 更加合适，在 Shader 中也可以看到具体使用方式。`stageFlags` 指定为 `VK_SHADER_STAGE_COMPUTE_BIT`。随后即可将这个 pipeline layout 保存供创建计算流水线时使用。

* Descriptor Set：同样，写 descriptor 的时候也需要将 `type` 指定为 `VK_DESCRIPTOR_TYPE_STORAGE_IMAGE`。

* Pipeline：同样，Shader 种类 `VK_SHADER_STAGE_COMPUTE_BIT` 即可。

  

### Command Buffer 录制

在这里我们为向 Compute Queue 提交的指令开一个不一样的 Command Buffer Pool，因为图形和计算队列家族（创建时的 `queueFamilyIndex` 参数）可能不是一个。随后创建出一个 Command Buffer。

这个 Command Buffer 仅仅在指定或更换图像处理的方式时重新录制。我们在录制该缓冲之前让其 `vkQueueWaitIdle` 一下我们之前创建出来的 Compute Queue，以防队列在执行这些指令时更改指令缓冲。开始录制之后直接绑定 Pipeline 和 Descriptor Sets，注意 `bindingPoint` 需要更换为 `VK_PIPELINE_BIND_POINT_COMPUTE`。

接下来使用 `vkCmdDispatch` 命令来“调度”一个计算任务的执行。该方法除了传入 Command Buffer 之外，还要传入 Work Group 的大小。由于我们之后在 Shader 中会设置 Local Size 为 16x16x1，因此 Work Group 即设置为 width / 16，height / 16 和 1，其中 width 和 height 为图片大小。

在极端条件下，Work Group 大小和 Local Size 可能会超过设备允许的最大值。可以通过查询设备 Limits 得到这些值。Work Group 大小每一维度至少 65535，因此对于那些配合图形渲染进行计算的任务应该绰绰有余。



### 同步

除了一直在使用的图形的提交、呈现同步机制外，我们还要注意以下几个需要同步的点：

* 由于计算任务也在只管绘制的 draw loop 中提交，因此不能保证下一个 loop 的时候上一个 loop 的 Compute Command Buffer 也执行完毕了。因此需要加 Fence 保证上一帧提交的计算任务已经执行完毕了再执行这帧的计算任务避免冲突。

  在提交 Compute Command Buffer 之前进行 wait 操作，并在之后提交时再次加上这个 fence：

  ```c++
  vkWaitForFences(device, 1, &compute.fence, VK_TRUE, UINT64_MAX);
  vkResetFences(device, 1, &compute.fence);
  // ...
  vkQueueSubmit(compute.queue, 1, &computeSubmitInfo, compute.fence));
  ```

* 由于计算任务和图形任务是并行执行的，在图形任务这边需要拿到计算任务这边处理后的图像时，需要保证计算任务这边确确实实处理完这张图像了，即写后读（RAW）。因此我们在提交图形这边的 Command Buffer 时需要创建一个 Image Memory Barrier，Access Mask 为 `VK_ACCESS_SHADER_WRITE_BIT -> VK_ACCESS_SHADER_READ_BIT`，同时 Pipeline Stage Mask 为 `VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT -> VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT`。将 `vkCmdPipelineBarrier` 命令录制进去即可。可以看出，这次使用 Memory Barrier 并没有转换图像的 Layout。队列时序图示例如下：

  ```
  Graphics *|---IA---VS---Rasterize---[x]  <stall>  [o]---FS---Assembly ...
  Compute  *|----------------Compute----------------[x] <- Barrier
  ```

  

* 每次切换处理方法时需要重新录制 Compute Command Buffer，此时我们希望 Compute Queue 中已经没有任务在使用这个命令缓冲我们才能对其进行重新录制。在重新录制前我们直接使用最简单的一种方法：`vkQueueWaitIdle(compute.queue)`。





